PRIORITY:
******I need to subscribe to CJMLS and Bright MLS for all municipal sales data
*** Go through and delete all unnecessary/unused functions
*** corrupted_files() not needed. Will use try-except block to catch corrupted data
*** duplicate_vector_check() wont be needed. Will remove all duplicated when python dict converted to
    pandas df and saved in sql
*** duplicate_eventlog_check() not needed Will remove all duplicated when python dict converted to
    pandas df and saved in sql
*** pdf_redundancy_check() not needed. Will remove all duplicated when python dict converted to
    pandas df and saved in sql
*** I HAVE TO CREATE A TABLE THAT HOLDS THE BASIC NJREALTOR DATA

125) Ensure cls.state_dictionary is pulling from the most recent Excel file/SQL table (DONE)
    - Ensure it pulling from the correct excel file (NOT NECESSARY)
    - Ensure cls.state_dictionary is doing what I want it to do (WORKS FINE)

The following functions need to be updated:
[year for year in range(2019,date.now().year + 1)]

1) __init__(self, session): (DONE)
    Dependencies: state_dictionary(), create_event_log(), njrdata()
    - __init__ will accept a request.Session variable which will be used throughout the script
    - self.session = session
    - Delete self.years and self.months
    - self.current_month, self.current_year = self.latest_nj_data()
    - self.run_number, self.last_ran_month, self.last_ran_year = self.latest_event_data()
    - Initialize a sqlalcchemy db connection
    - Delete the Scraper.state_dictionary(). Not being used anywhere
    - Make the main_dictionary an instance variable and rename self.njrdata
    - self.njrdata = {'City': [], 'County': [], 'Quarter': [], 'Month': [], 'Year': [], 'New Listings': [],
                         'New Listing % Change (YoY)': [], 'Closed Sales': [], 'Closed Sale % Change (YoY)': [],
                         'Days on Markets': [], 'Days on Market % Change (YoY)': [], 'Median Sales Prices': [],
                         'Median Sales Price % Change (YoY)': [], 'Percent of Listing Price Received': [],
                         'Percent of Listing Price Receive % Change (YoY)': [], 'Inventory of Homes for Sales': [],
                         'Inventory of Homes for Sale % Change (YoY)': [], 'Months of Supply': [],
                         'Months of Supplies % Change (YoY)': []
    }

2) extract_re_data():
     Dependencies: parse_pdfname(), good_data()
     - find the abspath of the pdfname and use that to open it
    - remove possible_corrupted_list, main_dict, year_dict as function variables
    - remove if pdfname in possible_corrupted_list:
        - I dont care if a corrupt file is seen. Surround whole process in try-except block
    - Just search lines[24:] if the town and county are in there. If not, raise Exception
    - Remove check_county() and pdf_redundancy_check()
    - Remove the county assertion
    - Remove re.error, AssertionError, and Exception from the except blocks
    - SORT THE DATA (NOT DONE)

3) good_data(): (DONE)
    - Remove assertion clause
    - Only accpet target, city, county, month1, year1 as function variables
    - Delete variable_list
    - Replace category_list
    - new function:
    category_list = ['New Listings', 'Closed Sales', 'Days on Markets', 'Median Sales Prices', 'Percent of Listing Price Received',
                     'Inventory of Homes for Sales', 'Months of Supply']
    function_list = [Scraper.find_new_listings, Scraper.find_closed_sales, Scraper.find_dom, Scraper.find_median_sales,
                     Scraper.find_percent_lpr, Scraper.find_inventory, Scraper.find_supply]


    month = Scraper.find_month(pdftext)
    self.njrdata['Month'].append(month)
    self.njrdata['Quarter'].append(Scraper.quarter(month))
    self.njrdata['Year'].append(Scraper.find_key_metrics(pdftext))
    self.njrdata['City'].append(city)

    for category, function in zip(category_list, function_list):
        data = function(pdftext)
        self.njrdata[category].append(data[0])
        self.njrdata[f'{category} % Change (YoY)'].append(data[1])

4) pdf_generator(): (DONE)
    -new_function:

    base_path = 'C:\\Users\\Omar\\Desktop\\Python Temp Folder'

    if pdfname is None:
        filenames = os.listdir(base_path)
    elif type(pdfname) is list:
        filenames = []
        try:
            for year, municipality in zip([y for y in range(2019,date.now().year + 1)], pdfname):
                search_directory = f'C:\\Users\\Omar\\Desktop\\Python Temp Folder\\PDF Temp Files\\{year}\\{municipality}'
                missing_files = os.listdir(search_directory)
                filenames.extend(missing_files)
        except FileNotFoundError:
            # More than likely there are no files downloaded for that year
            continue

    elif pdfname.endswith('.pdf'):
        filenames = os.listdir(base_path)
        filenames = filenames[filenames.index(pdfname) + 1:]

    for filename in filenames:
        if filename.endswith('.pdf'):
            yield filename
        else:
            continue

5) parsed_pdfname(): (DONE)
    - Do not return the town_directory
    - Only output town, county, month, year

6) fill_missing_data():
    - Update the Scraper.pdf_generator and self.extract_re_data() functions

7) data_na(): (DONE)
    - Only accept the town, month, and year variables
    - Replace category_list
    - Delete everything else
    - new function:

    category_list = ['New Listings', 'Closed Sales', 'Days on Markets', 'Median Sales Prices', 'Percent of Listing Price Received',
                     'Inventory of Homes for Sales', 'Months of Supply']

    self.njrdata['Month'].append(month)
    self.njrdata['Quarter'].append(Scraper.quarter(month))
    self.njrdata['Year'].append(year)
    self.njrdata['City'].append(town)
    self.njrdata['County'].append('N.A')
    self.njrdata['Month'].append(month)

    for category in category_list:
        self.njrdata[category].append(0)
        self.njrdata[f'{category} % Change (YoY)'].append(0.0)

8) connect2postgresql(): (DONE)
    - Will use sqlalchemy to create a connection

9) create_event_log(db_engine):
    - change function name to latest_event_data()
    - ***** Save most recent event log dict to sql
    - Accept the self.db_engine as a var since the classmethod cant directly access it
    - latest_event = pd.read_sql_table('event_log', engine=db_engine).loc[-1]
    - last_run_num = latest_event['Run Num']
    - last_date = latest_event['Latest Available Data'].split(' ')
    - Also have a try-except block that says if there's no data:
        return 1, September, 2019

    return last_run_num + 1, month2num(last_date[0]), int(last_date[1])

*** I have to create properties that can access the class data

10) njrdata(): (DONE)
    Dependencies: area_results(), current_data_avail()
    - Rename to latest_nj_data(self)
    - Remove all the Selenium code and use the session variable
    - new function:
        url = 'https://www.njrealtor.com/login.php?rd=10&passedURL=/goto.php?10kresearch=1&skipToken=1'
        response = session.get(url)

        if response.status_code == 200:
            page_source = response.text
            soup = BeautifulSoup(page_results, 'html.parser')
            self.area_results(soup)

         return self.current_data_avail(soup)

11) current_data_avail(): (DONE)

    def month2num(month): (DONE)
      # Switch the numbers and months around
      month_dict = {
          1: 'January', 2: 'February',
          3: 'March', 4: 'April',
          5: 'May', 6: 'June',
          7: 'July', 8: 'August',
          9: 'September', 10: 'October',
          11: 'November', 12: 'December'
        }

        return month_dict[month]

    current_results = soup.find('select', id="lmuTime").children
    current_results = list(current_results)
    target = current_results[2].get_text().split(' ')

    return month2num(target[0]), target[1]

12) njr10k(self, session): (NEED TO HANDLE IF THIS IS AN UPDATED RUN)
    Dependencies: latest_file(), self.towns(), self.create_url_and_pdfname()
    - Rename njr10k_downloads(self, **kwargs):
    - new function:

    def create_timeframe()

        timeframe = {}
        months = {'01': 'January', '02': 'February',
                             '03': 'March', '04': 'April',
                             '05': 'May', '06': 'June',
                             '07': 'July', '08': 'August',
                             '09': 'September', '10': 'October',
                             '11': 'November', '12': 'December'
                             }
        for y in [year for year in range(self.latest_ran_year, self.current_year + 1)]:
            if y == self.latest_ran_year:
                # May have to play with the month numbers
                month_start = list(months.keys()).index(self.latest_ran_month) + 1

                timeframe[y] = list(months.keys())[month_start:]
            else:
                timeframe[y] = list(months.keys())

        return timeframe

    base_url = 'http://njar.stats.10kresearch.com/docs/lmu/'
    params = {'src': 'Page'}

    for key, value in self.create_timeframe().keys():
        target_list = product(key, value, self.towns)
        for _, data in zip(trange(len(target_list)), target_list):
            pdf_url, new_filename = self.create_url_and_pdfname(base_url, data[0], data[1], data[2])
            Scraper.download_pdf(session, pdf_url, pdf_name, params_dict)

13) latest_file():
    - Rename last_downloaded_pdf()
    - Should be run in the main function
    - Refactor how to find last downloaded file based on metadata of the file,
    and how much data was supposed to be downloaded vs how much has been
    - Use the read_logger function to assist this function
    - Start the logger file with all the necessary run data to be parsed in the event of a script failure
    - In the event of program failure, use logger and last downloaded pdf to create the modified timeframe in njr10k()
    - Return the name of a pdf file or 'No pdfs available'

14) pandas2sql():
    - This will now be an instance method rather than static
    - Only arg will be **kwargs, table name will be a var that lives inside the function
    - Turn self.njrdata into pandas df inside function
    - Use Pandas and sqlalchemy to append df to currently existing table in PostgreSQL

15) read_logger():
    - Refactor to find the instance run data and last downloaded file

16) logger_decorator():
    - Only use on main(). Take logger off the other functions. Not necessary to use on multiple functions
    - Change the f_handler and c_handler logger levels to where the stream logger only displays WARNING levels or above
    and the file collects everything

17) waiting() & text_message():
    - Use outside of the main().
    Use in conjunction with Airflow to modify the state of when the pipeline runs and give updates

18) event_log_update(): (DONE)
    - Use this to update the event log. Accept the event_log var from the main to update it
    - Append the row to the table

The following functions need to be deleted:

1) check_county() (DONE)
2) pdf_redundancy() (DONE)
3) state_dictionary() (DONE)
4) check_results() (DONE)
5) update_njr10k() (DONE)
6) Create_sql_table() (DONE)

____________________________________________________________________________________________________________________________________
"""
        UPDATE - READ LOG FROM SQL DB

        Classmethod which is run during class initialization update the class variable "event log" with
        the class' run history and updates the class variable "no_of_runs". In the event there isn't a shelf
        file available with event log history, a new event log dictionary is created.
        :return: None
        """
        save_path = 'F:\\Python 2.0\\Projects\\Real Life Projects\\' \
                    'NJR Scrapper\\Saved Data\\NJ Scrapper Data Dictionary_v2.dat'
        if os.path.exists(save_path):
            os.chdir('F:\\Python 2.0\\Projects\\Real Life Projects\\NJR Scrapper\\Saved Data')
            try:
                with shelve.open('NJ Scrapper Data Dictionary_v2', writeback=True) as saved_data_file:
                    if saved_data_file['Event Log']:
                        cls.event_log = saved_data_file['Event Log']
                        runs_list = [i for i in cls.event_log.keys()]
                        Scraper.duplicate_eventlog_check()
                        cls.no_of_runs = runs_list[-1] + 1

                os.chdir('C:\\Users\\Omar\\Desktop\\Python Temp Folder')

            except KeyError:
                os.chdir('C:\\Users\\Omar\\Desktop\\Python Temp Folder')
                key_names = ['Run Type', 'Latest Available Data', 'Run Time', 'Run Date', 'Days Between Update']
                if cls.event_log == {}:
                    cls.event_log.setdefault(cls.no_of_runs, {})

                    for kn in key_names:
                        cls.event_log[cls.no_of_runs].setdefault(kn, '')

        """
        UPDATE
        EVENT LOG SHOULD BE A PANDAS DF AND THE NEW DATA SHOULD BE A PANDAS
        SERIES WHICH IS APPENEDED TO THE BOTTOM OF THE FULL LOG

        Instance method which updates the event log with runtime data of the most recent NJR10k download.
        Stores the type of downlaod/update which was run, the length of the download runtime, current date and
        length in time between the previous and current program runs
        :param name: Name of the function ran (njr10k or update_njr10k)
        :param run_time: time object which value is the download function's run time
        :param logger: logger function which will return event log to ther logger file
        :return: None
        """
        Scraper.event_log.setdefault(Scraper.no_of_runs, {'Run Type': '', 'Latest Available Data': '',
                                                          'Run Time': '', 'Run Date': '', 'Days Between Update': ''})
        Scraper.event_log[Scraper.no_of_runs]['Run Type'] = name
        Scraper.event_log[Scraper.no_of_runs]['Latest Available Data'] = Scraper.current_data
        Scraper.event_log[Scraper.no_of_runs]['Run Time'] = str(run_time)
        Scraper.event_log[Scraper.no_of_runs]['Run Date'] = time.ctime()
        Scraper.event_log[Scraper.no_of_runs]['Days Between Update'] = Scraper.daysuntilupdate()

        logger.info(f'New Event Log Created:\n'
                    f"\nRun Date: {Scraper.event_log[Scraper.no_of_runs]['Run Date']}"
                    f"\nRun #: {Scraper.no_of_runs}"
                    f"\nRun Type: {Scraper.event_log[Scraper.no_of_runs]['Run Type']}"
                    f"\nLatest Available Data: {Scraper.event_log[Scraper.no_of_runs]['Latest Available Data']}"
                    f"\nRun Time: {Scraper.event_log[Scraper.no_of_runs]['Run Time']}"
                    f"\nDays Between Update: {Scraper.event_log[Scraper.no_of_runs]['Days Between Update']}")

        message_body = '     NJ Scrapper     ' \
                       f"\n" \
                       f"\nEvent Log:" \
                       f"\nRun Date: {Scraper.event_log[Scraper.no_of_runs]['Run Date']}" \
                       f"\nRun #: {Scraper.no_of_runs}" \
                       f"\nRun Type: {Scraper.event_log[Scraper.no_of_runs]['Run Type']}" \
                       f"\nLatest Available Data: {Scraper.event_log[Scraper.no_of_runs]['Latest Available Data']}" \
                       f"\nRun Time: {Scraper.event_log[Scraper.no_of_runs]['Run Time']}" \
                       f"\nDays Between Update: {Scraper.event_log[Scraper.no_of_runs]['Days Between Update']}" \
                       f"\nFilename: {excelfile}"



_________________________________________________________________________________________________________________________________________


110) Clean all the strong and weak warning and format to PEP standards
72) ***Look into aiohttp for async http requesting
- asyncio is pretty difficult to use
- there's something about the async setup that mixing up the split function for the cities with duplicate names
- the download pdf function isnt downloading anything
73) asynio Exceptions to my try-except blocks which use the module
79) Check back in a few days to determine campaign registration status on Twilio.com or email
80) Create decorator functions for these tasks:
    ***Use functool.wrapper to keep the metadata
 - Logger decorator function for:
    ***Change the logger level depending on the function running if possible
    ***May need to have "logger" as an arg to use in the function
    - CreateZip (properly place logger messages in function)
45) If the program is killed due to an exception, I want to be able to rerun the program X number of times


