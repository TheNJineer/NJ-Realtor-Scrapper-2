
asyncio task list:
1) CreateZip
2) data_na
3) duplicate_vector_check
4) extract_re_data
5) good_data
6) njr10k
7) update_njr10k

Tasks in njr10k: (Completed)
- request Session
- parse city and weburl info (turn this into its own function) task1
- inputs needed:
 - base_url
 - city
 - city_list
 - year
 - month
 - params
- download pdf
***make sure all tasks are put into the task list (NOT COMPLETE)

Tasks in data_na (COMPLETED)
- assign values to all of the variables
- input values into main dictionary
- put values into full year dictionary if necessary

Tasks in duplicate_vector_check (COMPLETED)
#May not even be a necessary function anymore
- get length of cities in dictionary

Tasks in extract_re_data
- parse pdf name
- check if pdf in corrupted list
- extract pdf contents
- if pdf is good, run good_data (task1)
    - good_data will run duplicate_vector_check (task3)
- if corrupt, run data_na (task2)

***make sure all tasks are put into the task list (NOT COMPLETE)


55) Function that sends the data to an SQL Database
69) Add an except KeyboardInterupt and sys.exit() for this exception (COMPLETED)
72) ***Look into aiohttp for async http requesting
- asyncio is pretty difficult to use
- there's something about the async setup that mixing up the split function for the cities with duplicate names
- the download pdf function isnt downloading anything
73) asynio Exceptions to my try-except blocks which use the module
74) Ensure the check_results module is running as it should (may not be useful anymore) (COMPLETED)
75) Change the os.walk(base_path) to os.listdir(base_path) in read_logger (NOT NECESSARY)
The current code causes errors because its walking through all files and folders and trying to extract whats not there
https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory
79) Check back in a few days to determine campaign registration status on Twilio.com or email
80) Create decorator functions for these tasks:
    ***Use functool.wrapper to keep the metadata
 - Logger decorator function for:
    ***Change the logger level depending on the function running if possible
    ***May need to have "logger" as an arg to use in the function
    - CreateZip (properly place logger messages in function)

88) Check the duplicate vector function to see if its needed and works as intended (COMPLETED)
89) Make sure all the column datatypes are correct to calculate all stats
90) Make sure CreateZip runs as intended and the os.walk goes through all directories then set logger messages
91) Find a way to incorporate previous run date into run_main decorator
    Make sure the run_main function runs as intended
92) Mark all classmethods and staticmethods. Should some of the classmethods be @property?
    Go through main() and fix class and static method mentions:
        - check_results (COMPLETE)
        - create_event_log (COMPLETE)
        - CreateZip (COMPLETE)
        - data_na (COMPLETED)
        - daysuntilupdate (COMPLETE)
        - duplicate_vector_check (COMPLETE)
        - get_us_pw (COMPLETE)
        - OrganizeFiles (COMPLETE)
        - pdf_generator (COMPLETE)
        - quarter (COMPLETE)
        - text_message (COMPLETE)
        - waiting (COMPLETED)
45) If the program is killed due to an exception, I want to be able to rerun the program X number of times
86) Create a function to upload the GIS Data and create cloropleth maps and timeseries maps:

    *** Create a time-series graph on the map of New Jersey for these categories as well
    - Download the CSV or GeoJSON (using Geopandas) from https://gisdata-njdep.opendata.arcgis.com/datasets/newjersey::municipal-boundaries-of-nj/explore?location=40.084926%2C-74.743150%2C8.00&showTable=true
    - Make sure the GeoJSON index matches the df index (rename the 'MUN' column to 'City') and join/merge the dfs

    def cloropleth_maps():
        import geopandas
        import plotly.express as px

        new_jersey = geopandas.read_file(filename, index = 'MUN')
        new_jersey.rename(columns = {'MUN': 'City'}, inplace = True)
        new_jersey[[column for column in new_jersey.columns]].sort_values(by=['City'])
        ***I have to sort the dfs by Qtr and year before plotting

        quarters = ['Q1', 'Q2', 'Q3', 'Q4']
        for sheet in list_of_excel_workbook_sheets:
            df = pd.read_excel('NJ 10k Real Estate', sheet)
            ***Create different dfs which are sorted dfs for Q1-Q4
            df_list = [df[[ for q in quarters
            fig = px.cloropleth(

85) Clean the joined dfs and use matplotlib to create line and histogram charts for each county:
*** This should be a function
    - Median Sales Price
    - New Listings
    - Closed Sales
    - Days on Market
    - PoLPR
    - Inventory of Homes for Sales
    - Months of Supply

    *** Use the all_months df to create these new dfs then
    def njr10k_stats(self):
    1) Load the all_qtrs tab from the Excel sheet
    2) Create a two dfs where the mean, sum, stddev, for each quarter and the total year for each category is calculated:
        - Median Sales Price
        - New Listings
        - Closed Sales
        - Days on Market
        - PoLPR
        - Inventory of Homes for Sales
        - Months of Supply
        1) Load up the df
        2) Create two new empty dfs
            - by_qtr_stats & ytd_stats
            ***njr10k_stats & njr10k_updatestats need to be created. The first function will be to analyze what I already have then the second function will update the df going forward
            - Columns will be 'City', 'County', 'Date', 'Quarter', 'Year', 'Total New Listings', 'New Listings (Median)', 'New Listings (StdDev)',
            'Total Closed Sales', 'Closed Sales(Median)', 'Closed Sales(StdDev)', 'DoM (Median)', 'DoM(StdDev)', 'Sales Price(Median)',
            'Sales Price(StdDev)', 'PoLPR(Median)', 'PoLPR(StdDev)', 'Inventory of Homes for Sale(Median)', 'Inventory of Homes for Sale(StdDev)',
            'Monthly Supply(Median)', 'Monthly Supply(StdDev)'
            a) copy the main dfs city, county, date, year columns and place in ytd_stats. Filter for unique city names and save df
            b) copy the main dfs city, county, date, quarter and year columns and place in by_qtr_stats. Filter for unique qtr and save df
            c) Create a list of the column names from the main df then remove city, county, quarter, month, year
            d) Create a list of the unique city names
            e)  @logger_decorator
                def njr10k_stats(self, excelfile): # Try to use the absolute file path so I dont have to change the directory

                filename = 'NJ 10k Real Estate Stats ' + str(datetime.datetime.today().date()) + '.xlsx'

                temp_df = pd.read_excel(exelfile, sheet='All Months')

                # Empty df which will hold descriptive statistics by quarter
                by_qtr_stats = pd.DataFrame(df, columns=['City', 'County', 'Date', 'Quarter', 'Year', 'Total New Listings', 'New Listings (Median)', 'New Listings (StdDev)',
                'Total Closed Sales', 'Closed Sales(Median)', 'Closed Sales(StdDev)', 'DoM (Median)', 'DoM(StdDev)', 'Sales Price(Median)',
                'Sales Price(StdDev)', 'PoLPR(Median)', 'PoLPR(StdDev)', 'Inventory of Homes for Sale(Median)', 'Inventory of Homes for Sale(StdDev)',
                'Monthly Supply(Median)', 'Monthly Supply(StdDev)'])

                # Empty df which will hold descriptive statistics by for the YTD and previous years
                ytd_stats = pd.DataFrame(df, columns=['City', 'County', 'Date', 'Year', 'Total New Listings', 'New Listings (Median)', 'New Listings (StdDev)',
                'Total Closed Sales', 'Closed Sales(Median)', 'Closed Sales(StdDev)', 'DoM(Median)', 'DoM(StdDev)', 'Sales Price(Median)',
                'Sales Price(StdDev)', 'PoLPR(Median)', 'PoLPR(StdDev)', 'Inventory of Homes for Sale(Median)', 'Inventory of Homes for Sale(StdDev)',
                'Monthly Supply(Median)', 'Monthly Supply(StdDev)'])

                # Copy these four columns from the original df to the new dfs
                for i in ['City', 'County', 'Date', 'Year']:
                    by_qtr_stats[i] = temp_df[i]
                    ytd_stats[i] = temp_df[i]
                # Add the 'Quarter' column from the original df to the by_qtr_stats df
                by_qtr_stats['Quarter'] = temp_df['Quarter']

               # Drop the duplicate quarters for each city in the by_qtr_stats df and duplicate cities in the ytd_stats df
               by_qtr_stats.drop_duplicates(['City', 'Quarter', 'Year'])
               ytd_stats.drop_duplicates('City')

                # Create quarters, years, and city lists to use in a for loop
                quarters = ['Q1', 'Q2', 'Q3', 'Q4']
                years = temp_df['Year'].unique().to_list()
                city_list = ytd_stats['City'].to_list()
                temp_df_columns = temp_df.columns.to_list()
                ytd_stats_columns = ytd_stats.columns.to_list()
                by_qtr_stats_columns = by_qtr_stats.columns.to_list()

                for city_name in city_list:
                    for column_name in temp_df_columns: # This should be the columns from the temp_df. Some names dont match to new data frames. Figure out how to fix this
                        temp_df1 = temp_df[['City' == city_name]]
                        if column_name == 'New Listings':
                            ytd_stats.at[city_name,'Total New Listings'] = temp_df1[column_name].sum()
                            ytd_stats.at[city_name,'New Listings (Median)'] = round(temp_df1[column_name].median(), 3)
                            ytd_stats.at[city_name,'New Listings (StdDev)'] = round(temp_df1[column_name].std(), 3)

                        elif column_name == 'Closed Sales':
                            ytd_stats.at[city_name,'Total Closed Sales'] = temp_df1[column_name].sum()
                            ytd_stats.at[city_name,'Closed Sales(Median)'] = round(temp_df1[column_name].median(), 3)
                            ytd_stats.at[city_name,'Closed Sales(StdDev)'] = round(temp_df1[column_name].std(), 3)
                        elif column_name in ytd_stats_columns[ytd_stats_columns.index('DoM(Median)'):-1]:
                            ***Make sure the list is stopped at Monthly Supply(Median) or it will create an indexing error
                            ytd_stats.at[city_name,ytd_stats_columns[ytd_stats_columns.index(column_name)]] = round(temp_df1[column_name].median(), 3)
                            ytd_stats.at[city_name,ytd_stats_columns[ytd_stats_columns.index(column_name) + 1]] = round(temp_df1[column_name].std(), 3)

                        for q in quarters:
                            *** I'll need to create an exception for qtrs that aren't available yet
                            temp_df2 = temp_df1[['Quarter' == q]]
                            if column_name == 'New Listings':
                                by_qtr_stats.at[city_name,'Total New Listings'] = temp_df2[column_name].sum()
                                by_qtr_stats.at[city_name,'New Listings (Median)'] = round(temp_df1[column_name].median(), 3)
                                by_qtr_stats.at[city_name,'New Listings (StdDev)'] = round(temp_df1[column_name].std(), 3)
                            elif column_name == 'Closed Sales':
                                by_qtr_stats.at[city_name,'Total Closed Sales'] = temp_df2[column_name].sum()
                                by_qtr_stats.at[city_name,'Closed Sales(Median)'] = round(temp_df1[column_name].median(), 3)
                                by_qtr_stats.at[city_name,'Closed Sales(StdDev)'] = round(temp_df1[column_name].std(), 3)
                            elif column_name in by_qtr_stats_columns[by_qtr_stats_columns.index('DoM(Median)'):-1]:
                                ***Make sure the list is stopped at Monthly Supply(Median) or it will create an indexing error
                                by_qtr_stats.at[city_name,by_qtr_stats_columns[by_qtr_stats_columns.index(column_name)]] = round(temp_df2[column_name].median(), 3)
                                by_qtr_stats.at[city_name,by_qtr_stats_columns[by_qtr_stats_columns.index(column_name) + 1]] = round(temp_df2[column_name].std(), 3)

                with pd.ExcelWriter(filename) as writer:
                    logger.info(f'Now creating Dataframes for {by_qtr_stats} and {ytd_stats}')
                    ytd_stats..to_excel(writer, sheet_name='Stats YTD', index_col='City')
                    by_qtr_stats.to_excel(writer, sheet_name='Stats By Qtr', index_col='City')


    3) Create a function which creates line graphs for all categories by month and by qtr for all cities per county(sub x-axis will be by year)
        - Filter the new df by county
        def matplot_lines(self):

            by_qtr_stats = pd.read_excel('NJR Scrapper Data By Qtr Stats')
            years = temp_df['Year'].unique().to_list()
            current_year = years[-1]
            temp_df = by_qtr_stats.copy()
            temp1_df = temp_df[['Year' == years[-1]]]
            counties  = temp_df['County'].unique().to_list()
            c = len(counties)
            target_columns = temp_df.columns.to_list()
            # Remove the city, county, qtr and year columns from the list
            tc = len(target_columns)
            fig1, axs = plt.subplots(c, tc, figsize = (,)) # Figure1 will have 21 rows with 7 axes each and the major axis is by qtr of the target year
            fig2, axs = plt.subplots(c, tc, figsize = (,)) # Figure1 will have 21 rows with 7 axes each and the major axis is by months of the target year
            for idx, county in enumerate(counties):
                # I'm going to graph all of the years on one graph. For the current year, change the line type to dashes or different shapes
                temp_df1 = temp_df[['County' == county]]
                temp1_df1 = temp1_df1[['County' == county]]
                plt.fig1
                plt.title('Statistical Data By Quarter for ' + county)
                plt.fig2
                plt.title('Statistical Data By Month for ' + county)
                for idx1, column in enumerate(target_columns):
                    plt.fig1
                    plt.subplot(c, idx, idx1)
                    plt.plot(temp_df[['Year' < current_year]], temp_df[column], label = temp_df['City'])
                    # plt.plot(temp_df[['Year' == current_year]], temp_df[column], label = temp_df['City'])
                    plt.xlabel('Dates')
                    plt.ylabel(column)
                    plt.legend()
                    # Set the minor axes as the quarters
                    plt.fig2
                    plt.subplot(c, idx, idx1)
                    plt.plot(temp1_df1['Date'], temp1_df1[column], label = temp1_df1['City'])
                    plt.xlabel('Dates')
                    plt.ylabel(column)
                    plt.legend()

    4) Create a function which creates line graphs and histograms for all categories for YTD for all cities of interest (sub x-axis will be by year)
        def target_areas_graphs(self):
            ***DF of Cites which have Top 10 Highest Median Closed Sales from all counties and filter for Median Sales Price < $750,000 (Decide on price later)

            by_qtr_stats = pd.read_excel('NJR Scrapper Data By Qtr Stats')
            years = temp_df['Year'].unique().to_list()
            temp_df = by_qtr_stats.copy()
            temp1_df = temp_df[['Year' == years[-1]]]
            counties  = temp_df['County'].unique().to_list()
            top10list = []

            for county in counties:
                temp2_df = temp1_df[['County' == county]]
                temp3_df = temp2_df.nlargest(10, 'Closed Sales(Median)')
                temp4_df = temp3_df[['Sales Price(Median)' < 750000]]
                top10list.append(temp4_df)

            target_cities_df = pd.concat(top10list)

    5) Create a function which creates histograms for all categories for YTD for all counties (sub x-axis will be by year)
        - Filter the new df by county
        def matplot_all_graphs(self):
    6) Use Seaborn to create pairplots on a state level then county level
        - Filter the new df by county and year
        import Seaborn





previous_dir = os.getcwd()
os.chdir('F:\\Python 2.0\\Projects\\Real Life Projects\\NJR Scrapper\\Saved Data')
with shelve.open('NJ Scrapper Data Dictionary') as saved_data_file:
    main_dictionary = saved_data_file['Main Dictionary']
    full_year = saved_data_file['Full Year']

os.chdir(previous_dir)

excelfile = obj.pandas2excel(main_dictionary, full_year)

njr10k_stats(excelfile)





















Test String:
Key Metrics 2022 2023 Percent Change Thru 4-2022 Thru 4-2023 Percent Change
New Listings 25 13 - 48.0% 85 64 - 24.7%
Closed Sales 11 9 - 18.2% 46 37 - 19.6%
Days on Market Until Sale 36 42 + 16.7% 32 37 + 15.6%
Median Sales Price* $90,000 $101,000 + 12.2% $116,250 $95,000 - 18.3%
Percent of List Price Received* 107.8% 94.6% - 12.2% 99.7% 93.6% - 6.1%
Inventory of Homes for Sale 59 25 - 57.6% -- -- --
Months Supply of Inventory 4.8 2.0 - 58.3% -- -- --
Key Metrics 2022 2023 Percent Change Thru 4-2022 Thru 4-2023 Percent Change
New Listings 39 32 - 17.9% 134 131 - 2.2%
Closed Sales 19 34 + 78.9% 75 100 + 33.3%
Days on Market Until Sale 26 30 + 15.4% 39 46 + 17.9%
Median Sales Price* $70,000 $92,000 + 31.4% $72,334 $95,000 + 31.3%
Percent of List Price Received* 102.3% 99.8% - 2.4% 98.5% 97.6% - 0.9%
Inventory of Homes for Sale 86 61 - 29.1% -- -- --
Months Supply of Inventory 3.7 2.2 - 40.5% -- -- --
Key Metrics 2022 2023 Percent Change Thru 4-2022 Thru 4-2023 Percent Change
New Listings 0 0 -- 0 0 --
Closed Sales 0 0 -- 0 0 --
Days on Market Until Sale 0 0 -- 0 0 --
Median Sales Price* $0 $0 -- $0 $0 --
Percent of List Price Received* 0.0% 0.0% -- 0.0% 0.0% --
Inventory of Homes for Sale 0 0 -- -- -- --
Months Supply of Inventory 0.0 0.0 -- -- -- --
* Does not account for sale concessions and/or downpayment assistance.  |  Percent changes are calculated using rounded figures and can sometimes look extreme due to small sample size.Year to DateTownhouse-Condo April
Adult Community AprilYear to Date
Current as of May 20, 2023. All data from the multiple listing services in the state of New Jersey. Margin of error for reported statewide numbers is +/– 4% at a 95% confidence level. Report © 2023 ShowingTime.Local Market Update for April 2023
Provided by New Jersey REALTORS®
Camden City
Camden County
April Year to Date Single Family
$0$25,000$50,000$75,000$100,000$125,000$150,000$175,000
1-2010 1-2011 1-2012 1-2013 1-2014 1-2015 1-2016 1-2017 1-2018 1-2019 1-2020 1-2021 1-2022 1-2023Historical Median Sales Price by Property Type By Month
Single Family Townhouse-Condo Adult Community




8-4-23 Most recent updates (put in main file at home):
1) Put this in the for-loop in the main program to print better tables of the data

print(tabulate(main_dictionary['2022'], headers = 'keys', tablefmt = 'plain'))
print()
print(tabulate(main_dictionary['2023'], headers = 'keys', tablefmt = 'plain'))


4) New OrganizeFiles class (make sure to seperate .pdf and the logger files)



     | Traceback (most recent call last):
    |   File "F:/Python 2.0/Projects/Real Life Projects/NJRScrapper 8-17-23A.py", line 428, in download_pdf
    |     del city_list[city_list.index('/')]
    |                   ^^^^^^^^^^^^^^^^^^^^
    | ValueError: '/' is not in list

     | Traceback (most recent call last):
    |   File "F:/Python 2.0/Projects/Real Life Projects/NJRScrapper 8-17-23A.py", line 429, in download_pdf
    |     city_list.pop(city_list.index('/'))
    |                   ^^^^^^^^^^^^^^^^^^^^
    | ValueError: '/' is not in list

Traceback (most recent call last):
    |   File "F:/Python 2.0/Projects/Real Life Projects/NJRScrapper 8-17-23A.py", line 432, in download_pdf
    |     async with session_object.get(url3, params=params, stream=True) as reader, open(new_filename, 'wb') as writer:
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File "F:\Python 2.0\pythonProject\venv\NJR Scrapper\Lib\site-packages\aiohttp\client.py", line 922, in get
    |     self._request(hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs)
    | TypeError: ClientSession._request() got an unexpected keyword argument 'stream'

